{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & preprocess data. Fill the filepaths with the correct path to your Puffer data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sent_file = \"data/puffer/puffer_sent.csv\"\n",
    "video_acked_file = \"data/puffer/puffer_acked.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sent = pd.read_csv(video_sent_file)\n",
    "video_acked = pd.read_csv(video_acked_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge video_sent & video_acked on session_id, index, & video_ts\n",
    "merged_data = pd.merge(video_sent, video_acked, on=['session_id', 'index', 'video_ts'], suffixes=('_sent', '_acked'))\n",
    "# debugging: time_sent & time_acked not in merged data\n",
    "print(\"Columns in merged_data:\", merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for clarity\n",
    "merged_data.rename(columns={\n",
    "    'time (ns GMT)_sent': 'time_sent',\n",
    "    'time (ns GMT)_acked': 'time_acked'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamps to datetime (easier to handle)\n",
    "merged_data['time_sent'] = pd.to_datetime(merged_data['time_sent'], unit='ns')\n",
    "merged_data['time_acked'] = pd.to_datetime(merged_data['time_acked'], unit='ns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download duration in seconds\n",
    "merged_data['download_duration'] = (merged_data['time_acked'] - merged_data['time_sent']).dt.total_seconds()\n",
    "#throughput in Mbps\n",
    "merged_data['throughput'] = ((merged_data['size'] * 8) / merged_data['download_duration']) / 1000000\n",
    "#byres per transmission time\n",
    "merged_data['bytes_per_transmission_time'] = merged_data['size'] / merged_data['rtt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data['size'].mean())\n",
    "print(merged_data['size'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "features = ['size', 'rtt', 'throughput', 'in_flight', 'bytes_per_transmission_time']\n",
    "target = 'download_duration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try standardizing the data and see how that affects it? \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "merged_data[features] = scaler.fit_transform(merged_data[['size', 'rtt', 'throughput', 'in_flight', 'bytes_per_transmission_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows w missing vals in relevant features\n",
    "merged_data = merged_data.dropna(subset=features + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit: training taking a long time - using a subset of data: \n",
    "subset_fraction = 0.2\n",
    "merged_data = merged_data.sample(frac=subset_fraction, random_state=42)\n",
    "\n",
    "# split dataset\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1) # added n_jobs=-1 to use all CPU cores\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model performance\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc metrics\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Training MAE: {train_mae}, RMSE: {train_rmse}\")\n",
    "print(f\"Testing MAE: {test_mae}, RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# cross-validation\n",
    "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='neg_mean_absolute_error',  n_jobs=-1)\n",
    "print(f\"Cross-Validation MAE: {-np.mean(cv_scores)}\")\n",
    "\n",
    "# debugging: size of dataset & features\n",
    "print(f\"Dataset size (used for training and testing): {merged_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_collected_data = pd.read_csv(\"/path/to/your_twitch_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_collected_data.rename(columns={'dropped_packets': 'in_flight', 'total_tcp_len' : 'size'}, inplace=True)\n",
    "self_collected_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = self_collected_data[features]\n",
    "test_target = self_collected_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Inf or NaN values in the entire dataset\n",
    "print(test_features.isna().sum())  # Check for NaN values\n",
    "print((test_features == float('inf')).sum())  # Check for positive infinity\n",
    "print((test_features == float('-inf')).sum())  # Check for negative infinity\n",
    "\n",
    "# Optionally, check if any value is extremely large or too small for the float32 dtype\n",
    "# Check for values exceeding a threshold (e.g., 1e10 or -1e10)\n",
    "print((test_features > 1e10).sum())  # Check for values larger than 1e10\n",
    "print((test_features < -1e10).sum())  # Check for values smaller than -1e10\n",
    "\n",
    "# Find rows with large values in 'bytes_per_transmission_time'\n",
    "large_values = test_features[test_features['bytes_per_transmission_time'] > 1e10]  # Adjust threshold as needed\n",
    "print(large_values)\n",
    "\n",
    "# Find rows with small values in 'bytes_per_transmission_time'\n",
    "small_values = test_features[test_features['bytes_per_transmission_time'] < -1e10]  # Adjust threshold as needed\n",
    "print(small_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of the column excluding Inf\n",
    "median_value = test_features['bytes_per_transmission_time'].replace(float('inf'), np.nan).median()\n",
    "\n",
    "# Replace Inf values with the median\n",
    "test_features['bytes_per_transmission_time'] = test_features['bytes_per_transmission_time'].replace(float('inf'), median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_new = rf_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_mae = mean_absolute_error(test_target, test_pred_new)\n",
    "my_data_rmse = np.sqrt(mean_squared_error(test_target, test_pred_new))\n",
    "\n",
    "print(f\"My Data MAE: {my_data_mae}, RMSE: {my_data_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from trustee import RegressionTrustee\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "trustee = RegressionTrustee(expert=rf_model)\n",
    "trustee.fit(X_train, y_train)\n",
    "_, dt, _, score = trustee.explain()\n",
    "print(f\"Training score of pruned DT: {score}\")\n",
    "dt_y_pred = dt.predict(X_train)\n",
    "\n",
    "# plot a tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plot_tree(dt, feature_names=X_train.columns, filled=True, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
